{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuning/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 types of argument in the path\n",
      "A sample of file is /home/yuning/DL/Monument/Train/basic\n"
     ]
    }
   ],
   "source": [
    "file_path= \"/home/yuning/DL/Monument/Train\"\n",
    "file_type = [ os.path.join(file_path,image) for image in os.listdir(file_path)  ]\n",
    "print(f\"There are {len(file_type)} types of argument in the path\")\n",
    "print(f\"A sample of file is {file_type[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yuning/DL/Monument/Train/basic/Aug-134-0.jpg\n",
      "3 640 360\n"
     ]
    }
   ],
   "source": [
    "sample_path = [os.path.join(file_type[0],i) for i in os.listdir(file_type[0])][0]\n",
    "print(sample_path)\n",
    "channels,width,height = io.read_image(sample_path).size()\n",
    "knsize = [5,3]\n",
    "feature_dims = [32,64,128]\n",
    "zdim = 32\n",
    "print(channels,height,width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.eval of Sequential(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(2, 2), padding=(3, 3))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
      ")>\n",
      "<bound method Module.eval of Sequential(\n",
      "  (flatten): Flatten()\n",
      "  (dense1): Linear(in_features=7372800, out_features=128, bias=True)\n",
      "  (elu1): ELU(alpha=1.0)\n",
      "  (dense2): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (elu2): ELU(alpha=1.0)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "from src import nn\n",
    "g_e = nn.g_e(height,width,channels,device,knsize,zdim,feature_dims)\n",
    "print(g_e.convnet.eval)\n",
    "print(g_e.mlp.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 47, 82])\n"
     ]
    }
   ],
   "source": [
    "x_sample = io.read_image(sample_path).float().to(device)\n",
    "pred = g_e.convnet(x_sample)\n",
    "# pred_out = g_e(x_sample)\n",
    "print(pred.detach().cpu().size())\n",
    "# print(pred.detach().cpu().size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensors = []\n",
    "for imag_type in file_type:\n",
    "    image_files = [os.path.join(imag_type,image_name) for image_name in os.listdir(imag_type)]\n",
    "    sub_tensors = [io.read_image(file_imag)/255 for file_imag in image_files]\n",
    "    Tensors.append(sub_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllTensor = []\n",
    "for tensors in Tensors:\n",
    "    AllTensor.append( torch.stack([  i for i in tensors]))\n",
    "\n",
    "All_Tensor = torch.concat(AllTensor)\n",
    "print(All_Tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensordataset = TensorDataset(All_Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1024)\n",
    "train_dl = DataLoader(Tensordataset,batch_size=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "809fbcd9e2fe2dbd4f61147d4abea8d65907ef0a473d6f45e857e1996288032b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
