{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 09:19:40.537813: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-06 09:19:40.684960: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-06 09:19:41.259198: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.3/lib64${LD_LIBRARY_PATH:+:/usr/lib/cuda/include:/usr/lib/cuda/lib64:}\n",
      "2023-01-06 09:19:41.259263: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.3/lib64${LD_LIBRARY_PATH:+:/usr/lib/cuda/include:/usr/lib/cuda/lib64:}\n",
      "2023-01-06 09:19:41.259270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import keras.backend as K \n",
    "from keras import layers\n",
    "from keras.utils.image_utils import array_to_img \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global HEIGHT, WIDTH,CHANNEL,LATENT_DIM\n",
    "HEIGHT = 360; WIDTH = 640; CHANNEL =3 ; LATENT_DIM = 32; \n",
    "BATCH_SIZE = 2; LR_INIT = 1e-3; THRESHOLD = 1.5;\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AE_Encoder():\n",
    "    encoder_inputs = keras.Input(name=\"Encoder_inputs\",shape = (HEIGHT,WIDTH,CHANNEL))\n",
    "    x = layers.Conv2D(32,(5,5),strides=(2,2),padding =\"same\",name= \"conv_1\")(encoder_inputs)\n",
    "    x = layers.Conv2D(64,(3,3),strides=(2,2),padding =\"same\",name= \"conv_2\")(x)\n",
    "    x = layers.Conv2D(128,(3,3),strides=(2,2),padding =\"same\",name= \"conv_3\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128,activation=\"relu\",name=\"encoder_dense2\")(x)   \n",
    "    x = layers.Dense(LATENT_DIM,name= \"hyper_dense\")(x)\n",
    "    return keras.Model(inputs =encoder_inputs,outputs =x,name=\"ae_encoder\")\n",
    "\n",
    "\n",
    "def AE_Decoder():\n",
    "  latent_inputs= layers.Input(shape=(LATENT_DIM,),name=\"latent_input\")\n",
    "  x = layers.Dense(128,activation=\"relu\",name=\"decoder_dense1\")(latent_inputs)\n",
    "  x= layers.Dense(45*80*128,activation=\"relu\",name=\"decoder_dense3\")(x)\n",
    "  x = layers.Reshape((45,80,128))(x)\n",
    "  x = layers.Conv2DTranspose(128,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\",name=\"decoder_T2D_1\")(x)\n",
    "  x = layers.Conv2DTranspose(64,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\",name=\"decoder_T2D_2\")(x)\n",
    "  x = layers.Conv2DTranspose(32,(3,3),strides=(2,2),padding=\"same\",activation=\"relu\",name=\"decoder_T2D_3\")(x)\n",
    "  decoder_outputs = layers.Conv2D(CHANNEL,(3,3),strides=(1,1),padding=\"same\",name=\"aede_out\")(x)\n",
    "  return keras.Model(latent_inputs,decoder_outputs,name=\"ae_decoder\")\n",
    "\n",
    "\n",
    "## Try larger strides and kernel and more CNN layer to make sure when it is flatten the parameter wont be too much !\n",
    "def Encoder():\n",
    "    encoder_inputs = keras.Input(name=\"Encoder_inputs\",shape = (HEIGHT,WIDTH,CHANNEL))\n",
    "    x = layers.Conv2D(32,(5,5),strides=(2,2),padding =\"same\",name= \"conv_1\")(encoder_inputs)\n",
    "    x = layers.Conv2D(64,(3,3),strides=(2,2),padding =\"same\",name= \"conv_2\")(x)\n",
    "    x = layers.Conv2D(128,(3,3),strides=(2,2),padding =\"same\",name= \"conv_3\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128,activation=\"relu\",name=\"encoder_dense2\")(x)    \n",
    "    x = layers.Dense(LATENT_DIM,name= \"hyper_dense\")(x)\n",
    "    return keras.Model(inputs =encoder_inputs,outputs =x,name=\"encoder\")\n",
    "\n",
    "def feature_extractor():\n",
    "    input_layer = layers.Input(name=\"extractor_input\",shape=(HEIGHT,WIDTH,CHANNEL))\n",
    "    x = layers.Conv2D(32,(5,5),strides=(2,2),padding =\"same\",name= \"extractor_conv_1\",kernel_regularizer=\"l2\")(input_layer)\n",
    "    x = layers.LeakyReLU(name=\"extractor_leaky_1\")(x)\n",
    "    x = layers.BatchNormalization(name=\"extractor_norm_1\")(x)\n",
    "    x = layers.Conv2D(64,(3,3),strides=(2,2),padding =\"same\",name= \"extractor_conv_2\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.LeakyReLU(name=\"extractor_leaky_2\")(x)\n",
    "    x = layers.BatchNormalization(name=\"extractor_norm_2\")(x)\n",
    "    x = layers.Conv2D(128,(3,3),strides=(2,2),padding =\"same\",name= \"extractor_conv_3\",kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.LeakyReLU(name=\"extractor_leaky_3\")(x)\n",
    "    x = layers.BatchNormalization(name=\"extractor_norm_3\")(x)\n",
    "    return keras.Model(input_layer,x,name=\"feature_extractor\")\n",
    "\n",
    "def Descriminator(fe):\n",
    "  input_layer = layers.Input(shape=(HEIGHT,WIDTH,CHANNEL))\n",
    "  x = fe(input_layer)\n",
    "  x = layers.GlobalAveragePooling2D(name=\"glb_avg\")(x)\n",
    "    \n",
    "  x = layers.Dense(1,activation = \"sigmoid\",name=\"d_output\")(x)\n",
    "  return keras.Model(input_layer,x,name=\"discriminator\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 09:19:42.493603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 09:19:42.498484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 09:19:42.498907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 09:19:42.499953: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-06 09:19:42.500512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 09:19:42.501823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 09:19:42.502371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 09:19:42.966936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 09:19:42.967430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 09:19:42.967723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-06 09:19:42.967994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4008 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "K.clear_session\n",
    "global encod,decod,Encoder,d,AE\n",
    "encod = AE_Encoder()\n",
    "decod = AE_Decoder()\n",
    "Encoder = Encoder()\n",
    "feature_extractor = feature_extractor()\n",
    "d = Descriminator(feature_extractor)\n",
    "AE = keras.models.Sequential(name=\"AutoEncoder\")\n",
    "AE.add(encod)\n",
    "AE.add(decod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.compile(optimizer = keras.optimizers.Adam(learning_rate = LR_INIT),loss=keras.losses.BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adv loss is the way discriminator stimulate the ae to learn\n",
    "class AdvLoss(keras.layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(AdvLoss,self).__init__(**kwargs)\n",
    "    @tf.function\n",
    "    def call(self,x,mask=None):\n",
    "        feature_extractor = d.get_layer(\"feature_extractor\")\n",
    "        ori_feature = feature_extractor(x[0])\n",
    "        gan_feature = feature_extractor(x[1])\n",
    "        return tf.reduce_mean(\n",
    "                            tf.square( ori_feature - gan_feature  ))\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0][0],3)\n",
    "\n",
    "# CntLoss is the way of ae itself to learn\n",
    "class CntLoss(keras.layers.Layer):\n",
    "    def __init__(self,**kwargs) :\n",
    "        super(CntLoss,self).__init__(**kwargs)\n",
    "    @tf.function\n",
    "    def call(self,x,mask=None):\n",
    "        ori = x[0]\n",
    "        gan = x[1]\n",
    "        cnt_loss = tf.reduce_mean(\n",
    "                        tf.reduce_sum(\n",
    "                            keras.losses.MAE(ori,gan),\n",
    "                            axis = (1,2)))\n",
    "        return cnt_loss\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0][0],3)\n",
    "\n",
    "class EncLoss(keras.layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(EncLoss,self).__init__(**kwargs)\n",
    "    @tf.function\n",
    "    def call(self,x,mask=None):\n",
    "        # ae = ae_trainer.get_layer(\"AutoEncoder\")\n",
    "        encod = AE.get_layer(\"ae_encoder\")\n",
    "        ori = x[0]\n",
    "        gan = x[1]\n",
    "        enc_loss = tf.reduce_sum(\n",
    "                      keras.losses.MSE( Encoder(gan),encod(ori)))\n",
    "        return enc_loss\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0][0],3)\n",
    "\n",
    "def loss(y_true,y_pred):\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = layers.Input(name = \"input\",shape=(HEIGHT,WIDTH,CHANNEL))\n",
    "gan = AE(input_layer)\n",
    "\n",
    "adv_loss = AdvLoss(name = \"adv_loss\")([input_layer,gan])\n",
    "cnt_loss = CntLoss(name=\"cnt_loss\")([input_layer,gan])\n",
    "enc_loss = EncLoss(name=\"enc_loss\")([input_layer,gan])\n",
    "\n",
    "ae_trainer = keras.models.Model(input_layer,[adv_loss,cnt_loss,enc_loss],name=\"ae_trainer\")\n",
    "losses = {\"adv_loss\":loss,\"cnt_loss\":loss,\"enc_loss\":loss}\n",
    "lossWeights = {\"cnt_loss\":40.0,\"adv_loss\":1,\"enc_loss\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_trainer.compile(optimizer=keras.optimizers.Adam(learning_rate = LR_INIT),\n",
    "                   loss= losses,\n",
    "                   loss_weights=lossWeights,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.load(\"tfdata\")\n",
    "dataset = dataset.shuffle(100)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.repeat(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-06\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now().date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def LRschedule(Model):\n",
    "  lr  = Model.optimizer.learning_rate\n",
    "  # print(f\"lr of {Model.name}:{Model.optimizer.learning_rate.numpy()}\\t\")\n",
    "  New_lr = lr * 0.001**(1/1500)\n",
    "  K.set_value(Model.optimizer.learning_rate,New_lr)\n",
    "  print(f\"Deacyed :{Model.optimizer.learning_rate.numpy()}\\n\")\n",
    "\n",
    "def SaveModel(AE,Encoder,d,iter):\n",
    "  path_string = \"/content/drive/MyDrive/Monument/{}GANAE_128/{}epoch\".format(datetime.datetime.now().date(),iter)\n",
    "  path = os.path.join(path_string)\n",
    "  if os.path.exists(path) is False:\n",
    "    os.makedirs(path)\n",
    "    print(f\"Making dir {path}\")\n",
    "  AE.save(path +\"/AE.h5\")\n",
    "  AE.save_weights(path+\"/AE_weights.h5\")\n",
    "  Encoder.save(path+\"/Encoder.h5\")\n",
    "  Encoder.save_weights(path+\"/Encoder_weights.h5\")\n",
    "  d.save(path+\"/Discriminator.h5\")\n",
    "  d.save_weights(path+\"/Discriminator_weights.h5\")\n",
    "  print(\"Models at {} epoch saved!\\n\".format(iter))\n",
    "\n",
    "def SaveHistory(d_loss_History,g_loss_History,iter):\n",
    "      g_loss_history = np.array(g_loss_History)\n",
    "      d_loss_history = np.array(d_loss_History)\n",
    "      import pandas as pd \n",
    "      g_loss_dict = {\"cnt_loss\":g_loss_history[:,0],\"adv_loss\":g_loss_history[:,1],\"enc_loss\":g_loss_history[:,2],\"loss_unknown\":g_loss_history[:,3]}\n",
    "      d_loss_dict = {\"d_loss\":d_loss_history} \n",
    "      df_g = pd.DataFrame(g_loss_dict)\n",
    "      df_d = pd.DataFrame(d_loss_dict)\n",
    "      path_string = \"/content/drive/MyDrive/Monument/{}GANAE_128/{}epoch\".format(datetime.datetime.now().date(),iter)\n",
    "      path = os.path.join(path_string)\n",
    "      if os.path.exists(path) is False:\n",
    "        os.makedirs(path)\n",
    "        print(f\"Making dir {path}\")\n",
    "      df_g.to_csv(os.path.join(path,\"g_loss{}.csv\".format(iter)))\n",
    "      df_d.to_csv(os.path.join(path,\"d_loss{}.csv\".format(iter)))\n",
    "      print(\"Histories at {} epoch saved!\\n\".format(iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 09:19:44.598674: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 360, 640, 3)\n",
      "(2, 360, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "d_loss_History = []\n",
    "g_loss_History = []\n",
    "niter = 1\n",
    "for i in range(niter+1):\n",
    "  x = dataset.take(1)\n",
    "  d.trainable = True\n",
    "  fake_x = AE.predict(x,verbose = 0)\n",
    "  xt = iter(x).next(); xt = xt.numpy()\n",
    "  # print(xt)\n",
    "  print(fake_x.shape)\n",
    "  d_x = np.concatenate([xt,fake_x],axis=0)\n",
    "  d_y = np.concatenate([np.zeros(len(xt)),np.ones(len(fake_x))],axis=0)\n",
    "\n",
    "  d_loss = d.train_on_batch(d_x,d_y)\n",
    "  d.trainable = False\n",
    "  \n",
    "  ########### Training Generator ##########\n",
    "  g_loss = ae_trainer.train_on_batch(xt,xt)\n",
    "\n",
    "  ########### Appending Loss ###############  \n",
    "  d_loss_History.append(d_loss)\n",
    "  g_loss_History.append(g_loss)\n",
    "  \n",
    "  if i%50 == 0 and i != 0:\n",
    "        LRschedule(d)\n",
    "        LRschedule(ae_trainer)\n",
    "  ########### Loss Report every 10epochs ##########\n",
    "  if i%10 ==0:\n",
    "        print(f\"Epoch:{i},g_loss:{g_loss},d_loss={d_loss}\")\n",
    "        print(\"d is trainable{}\".format(d.trainable))\n",
    "\n",
    "  ########### Model Callbacks every 100epochs ##########\n",
    "  # if i == 50: \n",
    "  #     SaveModel(AE,Encoder,d,iter=i)\n",
    "  #     SaveHistory(d_loss_History,g_loss_History,iter=i)\n",
    "  if i%100==0 and i != 0:\n",
    "      SaveModel(AE,Encoder,d,iter=i)\n",
    "      SaveHistory(d_loss_History,g_loss_History,iter=i)\n",
    "\n",
    "\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11e05876bec2ad9b34c669d9dff61cc48fedec39522fd08af25791e3a216550b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
